{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    need to add eval() to train.py\n",
    "    need to create test.py similat to train,py\n",
    "    create testing_setup() in all_for_one.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import DataSet, train\n",
    "from hyparams import hyparams\n",
    "from models import CNN\n",
    "\n",
    "import torch\n",
    "from torch.utils import data as data_utils\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys, os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda: True\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print('use_cuda: {}'.format(use_cuda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_setup():\n",
    "    # data loader setup\n",
    "    train_dataset = DataSet('train')\n",
    "    test_dataset = DataSet('test')\n",
    "\n",
    "    train_data_loader = data_utils.DataLoader( train_dataset, batch_size=hyparams.batch_size, shuffle=True)\n",
    "    test_data_loader = data_utils.DataLoader( test_dataset, batch_size=hyparams.batch_size)\n",
    "\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    print(\"device: \",device)\n",
    "\n",
    "    # Model\n",
    "    model = CNN().to(device)\n",
    "\n",
    "    print('total trainable params {}'.format(sum(p.numel() for p in model.parameters() if p.requires_grad)))\n",
    "\n",
    "    optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad], lr=hyparams.lr)\n",
    "\n",
    "    loss_list = train(device, model, train_data_loader, test_data_loader, optimizer, total_epoch=hyparams.total_epoch)\n",
    "\n",
    "    print(\"The End - train\")\n",
    "    return loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(loss_l):\n",
    "    epochs = [i for i in range(len(loss_l))]\n",
    "\n",
    "    plt.subplot(2,2,1)\n",
    "    plt.plot(epochs, loss_l, 'r', label= 'Training loss')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dl_finalpro\\\\data\\\\archive\\\\trainlist.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\dl_project\\dl_finalpro\\one-for_all.ipynb Cell 6\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/dl_project/dl_finalpro/one-for_all.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdir\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mE:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mdl_project\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mdl_finalpro\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\\a\u001b[39;00m\u001b[39mrchive\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39mrainlist.csv\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/dl_project/dl_finalpro/one-for_all.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m loss_list \u001b[39m=\u001b[39m training_setup()\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/dl_project/dl_finalpro/one-for_all.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m losss \u001b[39m=\u001b[39m loss_list\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/dl_project/dl_finalpro/one-for_all.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m#    losss = [1.1991665897698238, 1.154235893282397, 1.1485114714194988, 1.1389241177460243, 1.107125158967643, 1.0911626774689247, 1.0870850743918583, 1.073838706674247, 1.0620280709759942, 1.0425515174865723, 1.0226404132514164, 1.0084421367480838, 1.0027588893627297, 0.9970466515113567, 0.9776576716324379, 0.9760608282582514, 0.9694907336399473, 0.9624767529553381, 0.9623742432429873, 0.9620455071843904]\u001b[39;00m\n",
      "\u001b[1;32me:\\dl_project\\dl_finalpro\\one-for_all.ipynb Cell 6\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/dl_project/dl_finalpro/one-for_all.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtraining_setup\u001b[39m():\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/dl_project/dl_finalpro/one-for_all.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m# data loader setup\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/dl_project/dl_finalpro/one-for_all.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     train_dataset \u001b[39m=\u001b[39m DataSet(\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/dl_project/dl_finalpro/one-for_all.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     test_dataset \u001b[39m=\u001b[39m DataSet(\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/dl_project/dl_finalpro/one-for_all.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     train_data_loader \u001b[39m=\u001b[39m data_utils\u001b[39m.\u001b[39mDataLoader( train_dataset, batch_size\u001b[39m=\u001b[39mhyparams\u001b[39m.\u001b[39mbatch_size, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32me:\\dl_project\\dl_finalpro\\train.py:28\u001b[0m, in \u001b[0;36mDataSet.__init__\u001b[1;34m(self, split)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, split):\n\u001b[1;32m---> 28\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdatas \u001b[39m=\u001b[39m get_lists(split)[\u001b[39m0\u001b[39m]\n\u001b[0;32m     29\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels \u001b[39m=\u001b[39m get_lists(split)[\u001b[39m1\u001b[39m]\n",
      "File \u001b[1;32me:\\dl_project\\dl_finalpro\\datasetup.py:16\u001b[0m, in \u001b[0;36mget_lists\u001b[1;34m(split)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_lists\u001b[39m(split):\n\u001b[0;32m     15\u001b[0m     labellist \u001b[39m=\u001b[39m []\n\u001b[1;32m---> 16\u001b[0m     filelist \u001b[39m=\u001b[39m []\n\u001b[0;32m     18\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m( list_dir \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39mlist.csv\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(split) ) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m     19\u001b[0m         iterf \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(f)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dl_finalpro\\\\data\\\\archive\\\\trainlist.csv'"
     ]
    }
   ],
   "source": [
    "dir = \"E:\\dl_project\\dl_finalpro\\data\\archive\\trainlist.csv\"\n",
    "\n",
    "loss_list = training_setup()\n",
    "losss = loss_list\n",
    "#    losss = [1.1991665897698238, 1.154235893282397, 1.1485114714194988, 1.1389241177460243, 1.107125158967643, 1.0911626774689247, 1.0870850743918583, 1.073838706674247, 1.0620280709759942, 1.0425515174865723, 1.0226404132514164, 1.0084421367480838, 1.0027588893627297, 0.9970466515113567, 0.9776576716324379, 0.9760608282582514, 0.9694907336399473, 0.9624767529553381, 0.9623742432429873, 0.9620455071843904]\n",
    "plot(losss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
